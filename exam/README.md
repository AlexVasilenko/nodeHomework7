# Экзаменационное задание

## Организационные моменты.

1. Для выполнения экзамена, заведите отдельный репозиторий. 

2. Оба приложения должны "жить" в этом одном репозитории.
 
3. Дедлайн сдачи экзамена  *20 февраля 10:00*.

4. Для того чтобы сдать готовую работу, [заполните эту форму](https://goo.gl/forms/z4RVBTs4AySWb30H3).

5. По всем вопросам вы можете писать в `slack #help`.

----

## 1. Scraper API

Разработать веб-сервис для скрейпинга сайтов.

У API есть 3 endpoint'a:

### Сбор данных

`GET /api/search/?url=https%3A%2F%2Fgoogle.com&element=h2&level=3`

1. На вход сервис принимает адрес стартовой страницы произвольного сайта (get-параметр `url=`).

2. Сервис выкачивает указанную страницу и собирает содержимое DOM-элеметов которые были указаны в get параметре `element=`.

3. Сервис собирает с указанной страницы ссылки, фильтрует те, что принадлежат текущему домену и рекурсивно выкачивает найденные страницы.
В этой части следует предусмотреть фильтрацию якорей, уже посещенных ссылок, ссылок, не являющихся страница (mailto:, etc.). 
Также необходимо ограничить глубину обхода с помощью конфигурационного файла или не обязательного get-параметра `level=`.

4. На всех найденных страницах необходимо собрать содержимое указанных в п. 2 DOM-элементов и сохранить их в JSON-структуре в *Redis*.
Формат JSON следует определить самостоятельно.
Например:
`[ 'заголовок1', 'заголовок2', 'заголовок3', 'заголовок4 <span>очень</span> длинный' ]`

5. Перед тем как начинать обход сайта, необходимо проверить в Redis'e есть ли уже результаты поиска с такими же входящими параметрами `url, element, level`.
Если они уже существуют, то отдаем сразу из Redis'а, если нет, то выполняем поиск и сохраняем в Redis.

6. После отработки запроса необходимо отправить ответ пользователю с JSONом с данными или с ошибкой.

7. Необходимо поставить TTL для записей в Redis сроком в `1 день`, после этого они должны автоматически удаляться.

8. При каждом обращении к конкретной записи в Redis, необходимо продлевать TTL еще на `1 день`.

### Получение истории поисков

`GET /api/search/list`

1. Энпоинт должен возвращать JSON с историей совершенных ранее поисков, которые будут содержать параметры исходного запроса.
  
  Например:
  
```json
[
    {
        "url": "https://google.com",
        "level": 3,
        "element": "h2"
    },
    {
        "url": "https://google.com",
        "level": 2,
        "element": "h1"
    },
    {
        "url": "http://frontend-science.com",
        "level": 3,
        "element": "h4"
    }
]
```

История не должна изменяться или удаляться если запись с кэшем запроса была удалена из Redis'a.

### Удаление сохраненных поисков

`DELETE /api/search/?url=https%3A%2F%2Fgoogle.com&element=h2&level=3`


1. При запросе необходимо удалять соответствующую запись из Redis'a если она там существует.

2. Если такой записи не существует в Redis'e то возвращаем `status 404`. 

## 2. Swagger

Необходимо создать интерактивную Swagger документацию по 3м энпоинтам API

## 3. CLI

Необходимо создать отдельное CommandLine приложение для взаимодействия с Scraper API.
 
Примеры запуска тулзы

```

scraper --url http://google.com --level 4 --element h2

scraper -u http://google.com -l 4 -e h2

```

CL tool по http обращается к Scraper API и возвращает результат в консоль пользователю.

## 4. Tests

1. Написать тесты с моками данных и подменой сетевых запросов для тестирования Scraper API.

2. Вычислить покрытие кода тестами.

## 5. Templates (Optional)

1. По адресу `/` у Scraper API отдаем страницу с формой которая содержит:
    * URL `[text intput]`
    * Level `[select field (from 1 to 5)]`
    * Element `[text intput]`
    * Submit `[submit button]`
    
2. При сабмите формы делаем AJAX'ом запрос в Scraper API например `GET /api/search/?url=https%3A%2F%2Fgoogle.com&element=h2&level=3`.

3. Полученные данные выводим в виде html таблицы или списка.

## 6. Deployment (Optional)

1. Задеплоить приложение в Heroku.

2. Для Redis'a можно поставить этот плагин https://elements.heroku.com/addons/heroku-redis

----
## Links (использование предлагается, но не требуется)

1. В качестве сервера можно использовать [express](https://www.npmjs.com/package/express) (http://expressjs.com/).
2. В качестве модуля для выкачивания страниц сайтов — [got](https://www.npmjs.com/package/got).
3. Не используйте регулярные выражения для разбора HTML. Возьмите HTML-парсер. Можно воспользоваться [htmlparser2](https://www.npmjs.com/package/htmlparser2) (чистые данные) либо [cheerio](https://www.npmjs.com/package/cheerio) («со вкусом jQuery»).
4. В принципе не используйте регулярные выражения без крайней необходимости, не [будите Zalgo](http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags#answers-header)! Используйте модуль [url](https://nodejs.org/api/url.html) для разбора найденных урлов.
5. Не забывайте про логгирование. Подойдет любой из рассмотренных на лекции модулей.
6. При написании тестов потребуется подменять сетевые запросы. Здесь может помочь [nock](https://www.npmjs.com/package/nock)
7. Для подсчета покрытия можно использовать [istanbul](https://www.npmjs.com/package/istanbul).
